{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78423be7",
   "metadata": {},
   "source": [
    "# Hacker News Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda35a9a",
   "metadata": {},
   "source": [
    "This investigation looks at a dataset of news/interest stories from 2014. Using this dataset, a sequence of basic natural language processing tasks are run using an implementation of a pipeline. The goal is to find the top 100 keywords of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014.\n",
    "\n",
    "The investigation demonstrates the use of a task-based data pipeline, which orders tasks using a directed acyclic graph. The pipeline is used to **filter, clean, aggregate, and summarize data** in a sequence of tasks that will apply these transformations. Some tasks are written as 'generator' functions that successive tasks can use to iterate through values, instead of passing through all data at once.\n",
    "\n",
    "The DAG class and Pipeline class have been implementated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8cbf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See task_pipeline.py\n",
    "import task_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187b86e",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The starting data comes from a Hacker News (HN) API that returns JSON data of the top stories in 2014. Hacker News is a link aggregator website that users vote up stories that are interesting to the community. It is similar to Reddit, but the community only revolves around on computer science and entrepreneurship posts.\n",
    "\n",
    "A list of JSON posts from the website in saved to a file: hn_stories_2014.json. The JSON file contains a single key stories, which contains a list of stories (posts). The following keys are the ones we will examine:\n",
    "\n",
    "| Label | Description |\n",
    "| :--- | :--- |\n",
    "| created_at | A timestamp of the story's creation time. |\n",
    "| created_at_i | A unix epoch timestamp. |\n",
    "| url | The URL of the story link. |\n",
    "| objectID | The ID of the story. |\n",
    "| author | The story's author (username on HN). |\n",
    "| points | The number of upvotes the story had. |\n",
    "| title | The headline of the post. |\n",
    "| num_comments | The number of a comments a post has. |\n",
    "\n",
    "\n",
    "### Example JSON\n",
    "\n",
    "```\n",
    "{\n",
    "    \"story_text\": \"\",\n",
    "    \"created_at\": \"2014-05-29T08:23:46Z\",\n",
    "    \"story_title\": null,\n",
    "    \"story_id\": null,\n",
    "    \"comment_text\": null,\n",
    "    \"created_at_i\": 1401351826,\n",
    "    \"url\": \"http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/\",\n",
    "    \"parent_id\": null,\n",
    "    \"objectID\": \"7815285\",\n",
    "    \"author\": \"Leynos\",\n",
    "    \"points\": 1,\n",
    "    \"title\": \"Making Twitter Easier to Use\",\n",
    "    \"_tags\": [\n",
    "        \"story\",\n",
    "        \"author_Leynos\",\n",
    "        \"story_7815285\"\n",
    "    ],\n",
    "    \"num_comments\": 0,\n",
    "    \"_highlightResult\": {\n",
    "        \"story_text\": {\n",
    "            \"matchedWords\": [],\n",
    "            \"value\": \"\",\n",
    "            \"matchLevel\": \"none\"\n",
    "        },\n",
    "        \"author\": {\n",
    "            \"matchedWords\": [],\n",
    "            \"value\": \"Leynos\",\n",
    "            \"matchLevel\": \"none\"\n",
    "        },\n",
    "        \"url\": {\n",
    "            \"matchedWords\": [],\n",
    "            \"value\": \"http://bits.blogs.nytimes.com/2014/05/28/making-twitter-easier-to-use/\",\n",
    "            \"matchLevel\": \"none\"\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"matchedWords\": [],\n",
    "            \"value\": \"Making Twitter Easier to Use\",\n",
    "            \"matchLevel\": \"none\"\n",
    "        }\n",
    "    },\n",
    "    \"story_url\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62323377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this variable to True to run a set of tests going throughout the notebook:\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2853b",
   "metadata": {},
   "source": [
    "## Task 1: Read in the JSON data\n",
    "The task will output a list of stories from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6bdf258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Pipeline: \n",
    "pipeline = task_pipeline.Pipeline()\n",
    "\n",
    "# Add the first tast - to read the JSON file.\n",
    "import json\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r', encoding='UTF-8') as file:\n",
    "        stories = json.loads(file.read())\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e513a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this:\n",
    "if testing:\n",
    "    stories = file_to_json()\n",
    "    for key,value in stories.items():\n",
    "        print(value[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ded298",
   "metadata": {},
   "source": [
    "## Task 2: Read in the JSON data\n",
    "This task creates a generator function to iterate through the stories which have:\n",
    "- More than 0 points/votes from other users\n",
    "- 1 or more comments\n",
    "- Are not a question (starting with 'Ask HN') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca6f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def generator():\n",
    "        for x in stories['stories']:\n",
    "            if x['points'] > 0 and x['num_comments']>1 and not x['title'].startswith('Ask HN'):\n",
    "                yield x\n",
    "        return\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e79ac290",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    generator = filter_stories(stories)\n",
    "# for i, x in enumerate(generator()):\n",
    "#     print (x)\n",
    "#     break\n",
    "# print(i)\n",
    "# print(len(stories['stories']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921bb18",
   "metadata": {},
   "source": [
    "## Task 3: Read in the JSON data\n",
    "Using the generator from task 2, iterate through the stories we are interested in and build into CSV format. To do this, the ```build_csv``` helper is used to write the file to the destination (which might be disk, or could be an ```io.stringIO()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "755c8c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import itertools\n",
    "def build_csv(lines, header=None, file=None):\n",
    "\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories_enumerator):\n",
    "    stories2 = []\n",
    "    for story in stories_enumerator():\n",
    "        created_at = datetime. strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        stories2.append([story['objectID'], created_at, story['url'], story['points'], story['title']])\n",
    "\n",
    "    #print (stories2[0])\n",
    "    file = build_csv(stories2,\n",
    "                     header=['objectID', 'created_at', 'url', 'points', 'title'],\n",
    "                     file=open('test.csv','w+',newline='',encoding='utf-8'))\n",
    "    # This file could be implemented through io.StringIO()\n",
    "    # The disk file works better on my system and allows us to see what the middle step is\n",
    "    # but memory-based may be better for production (if not too big!)\n",
    "    return file\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73365e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    file = json_to_csv(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3661e3",
   "metadata": {},
   "source": [
    "## Task 4: Extract the titles form the stories\n",
    "The task will outputs a generator to iterate through the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bce499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(file):\n",
    "    def titles():\n",
    "        for row in reader:\n",
    "            yield row[index]\n",
    "        return\n",
    "\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)\n",
    "    #print (header)\n",
    "    #headings = header.split(',')\n",
    "    index = 0\n",
    "    for h in header:\n",
    "        if h =='title':\n",
    "            #print(\"Title index is:\", index)\n",
    "            break\n",
    "        index += 1\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b82d106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    file.seek(0)\n",
    "    title_generator = extract_titles(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5de1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    for x in title_generator():\n",
    "        print(x)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e27b51",
   "metadata": {},
   "source": [
    "## Task 5: Clean up the Titles\n",
    "Before parsing for the keywords, this task cleans out all punctuation and converts everything to lower case. The resulting clean text is returned as a generator function again to allow interation through the titles.\n",
    "\n",
    "To clean the strings, we will use the string.maketrans() method, with a useful list of punctuation symbols from the ```string``` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfdd9f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "if testing:\n",
    "    print(string.punctuation) # A useful list of punctuation symbols.\n",
    "    # Create a 'translation' mapping.\n",
    "    # All items in the 3rd argument are changed to None.\n",
    "    # This is fast - it's implemented using mapping in a C string.\n",
    "    strip_punctuation = str.maketrans('', '', string.punctuation)\n",
    "    print('hello! how are you?'.translate(strip_punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e8f45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    # Generator to return cleaned up titles:\n",
    "    def cleaned_titles():\n",
    "        for title in titles():\n",
    "            yield title.lower().translate(strip_punctuation)\n",
    "        return\n",
    "\n",
    "    # Create a 'translation' mapping to be used in the generator above.\n",
    "    # All items in the 3rd argument are changed to None.\n",
    "    # This is fast - it's implemented using mapping in a C string.   \n",
    "    strip_punctuation = str.maketrans('', '', string.punctuation)\n",
    "    return cleaned_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cf51fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    cleaned_titles = clean_titles(title_generator())\n",
    "    for t in cleaned_titles():\n",
    "        print(t)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae26dbcd",
   "metadata": {},
   "source": [
    "## Task 6: Build a Dictionary of Keywords\n",
    "We also have a list of 'stop words' from ```stop_words.py``` - which are common words we want to ignore.\n",
    "The task will output a word frequency dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aea1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import stop_words\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(cleaned_titles):\n",
    "    word_freq_dict = {}\n",
    "    for title in cleaned_titles():\n",
    "        for word in title.split(' '):\n",
    "            # Add to the count if it is not in our list of common words (stop_words)\n",
    "            # and we test for zero length, as multiple spaces are split this way.\n",
    "            if word not in stop_words and len(word) > 0:\n",
    "                word_freq_dict[word] = word_freq_dict.get(word, 0) + 1\n",
    "\n",
    "    return word_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d2d89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_keyword_input():\n",
    "    yield 'hello you horse exactly one day  today horse   to go before the end the end in nigh and the horse is neigh today '\n",
    "    return\n",
    "\n",
    "if testing:\n",
    "    build_keyword_dictionary(test_keyword_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "997f1b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    word_freq_dict = build_keyword_dictionary(cleaned_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bad9f7",
   "metadata": {},
   "source": [
    "## Task 7: List our the top 100 Keywords\n",
    "The task will output a list of the top 100 keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67987321",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_100_keywords(word_freq_dict):\n",
    "    # Sort the dictionary and put the results (tuples) into a list \n",
    "    top_100 = list(sorted(word_freq_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "    # return only the top 100:\n",
    "    return top_100[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46fbc56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    x = top_100_keywords(build_keyword_dictionary(test))\n",
    "    print(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fb93ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    x = top_100_keywords(word_freq_dict)\n",
    "    print(*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d9021",
   "metadata": {},
   "source": [
    "## Task 8: Read in the JSON data\n",
    "The task will add a simple 'progress' indication into the pipeline task list after csv creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0deeaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'progress' indicator into the pipeline after csv creation:\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def update_progress(file):\n",
    "    print(\"Finished conversion to temp csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f107df6",
   "metadata": {},
   "source": [
    "## Run the Pipeline\n",
    "This will now run the whole pipeline. The result (top 100 keywords) is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed427101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished conversion to temp csv\n",
      "[('new', 498), ('google', 481), ('bitcoin', 298), ('app', 295), ('web', 259), ('startup', 241), ('data', 240), ('open', 227), ('facebook', 219), ('code', 207), ('using', 202), ('programming', 188), ('use', 181), ('video', 180), ('free', 179), ('game', 167), ('time', 162), ('javascript', 156), ('apple', 154), ('ios', 153), ('source', 152), ('microsoft', 149), ('software', 147), ('make', 146), ('like', 146), ('internet', 145), ('2013', 140), ('world', 139), ('2014', 139), ('tech', 138), ('way', 136), ('github', 134), ('c', 132), ('python', 132), ('work', 130), ('people', 129), ('apps', 127), ('windows', 127), ('project', 123), ('twitter', 121), ('security', 120), ('pdf', 119), ('released', 119), ('yc', 117), ('1', 116), ('language', 114), ('dont', 114), ('vs', 113), ('users', 112), ('android', 112), ('email', 112), ('linux', 111), ('mobile', 106), ('startups', 106), ('news', 105), ('ceo', 104), ('better', 102), ('just', 102), ('api', 102), ('does', 101), ('service', 100), ('says', 100), ('simple', 99), ('online', 99), ('design', 98), ('developer', 95), ('help', 95), ('built', 94), ('hacker', 94), ('best', 94), ('build', 93), ('heartbleed', 93), ('day', 92), ('big', 90), ('search', 89), ('need', 89), ('years', 88), ('money', 87), ('10', 87), ('2', 87), ('amazon', 86), ('life', 86), ('nsa', 85), ('job', 85), ('developers', 84), ('computer', 84), ('2048', 84), ('development', 83), ('year', 83), ('site', 82), ('mozilla', 82), ('tool', 81), ('want', 81), ('learn', 81), ('good', 80), ('silicon', 80), ('browser', 79), ('3', 78), ('future', 78), ('building', 78)]\n"
     ]
    }
   ],
   "source": [
    "x = pipeline.run()\n",
    "print(x[top_100_keywords])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ea96a",
   "metadata": {},
   "source": [
    "## Commentary\n",
    "The pipeline generated an interesting list of 'top-of-mind' words from 2014. Noticably, we have **bitcoin** and **Heartbleed**. People were talking more about **JavaScript** than **Python**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac488c",
   "metadata": {},
   "source": [
    "## Further development ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf2a76",
   "metadata": {},
   "source": [
    "- Rewrite the way Pipeline class does its 'output' to save a file of the output for each task to 'checkpoint' tasks. In the case that one task fails in the pipeline, it can be restarted from the 'checkpoint'.\n",
    "- Use spaCy or the [nltk package](http://www.nltk.org/) for more natural language processing tasks.\n",
    "- Fetch the lastest data from Hacker News directly from [a JSON API](https://hn.algolia.com/api) and process this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
